# 1.5 Success Metrics & Pending SLO Thresholds

## Migration Success Metrics

These metrics determine whether the Auth and AI service extraction is successful.

### Functional Metrics

| Metric                           | Target                                                 | How to Verify                               |
| -------------------------------- | ------------------------------------------------------ | ------------------------------------------- |
| Auth endpoints respond correctly | All 6 endpoints return same responses as monolith      | Integration tests against extracted service |
| AI endpoints respond correctly   | All 3 endpoints return same responses as monolith      | Integration tests against extracted service |
| JWT tokens are interoperable     | Tokens issued by Auth Service work across all services | Cross-service auth test                     |
| Rate limiting works across pods  | Redis-backed limits enforced globally                  | Multi-request test exceeding limit          |
| Admin allowlist gates AI access  | Non-admin users get 403 on AI endpoints                | Test with admin and non-admin users         |
| Registration auto-joins #general | New users appear in #general after register            | Register + check membership                 |
| media-storage auth verification  | `GET /auth/me` still works for media-storage           | Upload test through media-storage           |
| Frontend login/register flow     | No changes needed in frontend                          | Manual E2E test                             |

### Operational Metrics

| Metric                         | Target                                         | How to Verify                             |
| ------------------------------ | ---------------------------------------------- | ----------------------------------------- |
| K3s cluster is stable          | No unscheduled restarts in 24h                 | `kubectl get events`                      |
| Argo CD syncs successfully     | All apps show "Synced" + "Healthy"             | Argo CD UI dashboard                      |
| Ingress routes correctly       | `/auth/*` → Auth Service, `/ai/*` → AI Service | `curl` tests against ingress              |
| PostgreSQL accepts connections | Auth Service connects without errors           | Health check endpoint + logs              |
| Redis accepts connections      | AI Service rate limiting works                 | Rate limit test + `redis-cli ping`        |
| Zero-downtime deployment       | Rolling update with no 5xx errors              | Deploy new version while sending requests |

### Performance Metrics (Baseline)

| Metric                   | Current Baseline (Monolith)      | Target (Microservices)     |
| ------------------------ | -------------------------------- | -------------------------- |
| Auth login latency (p50) | TBD — measure before migration   | ≤ current + 50ms overhead  |
| Auth login latency (p99) | TBD — measure before migration   | ≤ current + 100ms overhead |
| AI query latency (p50)   | TBD — dominated by Anthropic API | ≈ current (API-bound)      |
| AI query latency (p99)   | TBD — dominated by Anthropic API | ≈ current (API-bound)      |
| Registration throughput  | TBD — measure before migration   | ≥ current                  |

> **Action required**: Measure current baselines before starting migration. Use `wrk` or `hey` to benchmark auth and AI endpoints on the monolith.

---

## SLO Definitions (Thresholds TBD)

The following SLOs are defined structurally. **Custom threshold values are pending** and will be set after baseline measurement.

### Auth Service SLOs

| SLO          | Indicator                           | Threshold            | Window    |
| ------------ | ----------------------------------- | -------------------- | --------- |
| Availability | % of successful responses (non-5xx) | TBD (e.g., 99.9%)    | 30 days   |
| Latency      | p99 response time for `/auth/login` | TBD (e.g., < 500ms)  | 30 days   |
| Error rate   | % of 5xx responses                  | TBD (e.g., < 0.1%)   | 30 days   |
| Throughput   | Requests per second sustained       | TBD (e.g., > 50 RPS) | Peak hour |

### AI Service SLOs

| SLO                 | Indicator                                           | Threshold                    | Window  |
| ------------------- | --------------------------------------------------- | ---------------------------- | ------- |
| Availability        | % of successful responses (non-5xx)                 | TBD (e.g., 99.5%)            | 30 days |
| Latency             | p99 response time for `/ai/query` (excl. Anthropic) | TBD (e.g., < 200ms overhead) | 30 days |
| Error rate          | % of 5xx responses (excl. Anthropic errors)         | TBD (e.g., < 1%)             | 30 days |
| Rate limit accuracy | % of correctly enforced rate limits                 | TBD (e.g., 100%)             | 30 days |

### Infrastructure SLOs

| SLO               | Indicator                        | Threshold         | Window  |
| ----------------- | -------------------------------- | ----------------- | ------- |
| K3s uptime        | Cluster node availability        | TBD (e.g., 99.9%) | 30 days |
| PostgreSQL uptime | Database connection success rate | TBD (e.g., 99.9%) | 30 days |
| Redis uptime      | Cache/rate-limit availability    | TBD (e.g., 99.9%) | 30 days |
| Argo CD sync      | % of successful GitOps syncs     | TBD (e.g., 99%)   | 30 days |

---

## Next Steps for SLO Finalization

1. **Baseline measurement**: Run load tests against current monolith to establish latency, throughput, and error rate baselines
2. **Set thresholds**: Based on baselines + business requirements, fill in TBD values
3. **Implement monitoring**: Deploy metrics collection (Prometheus or similar) to track SLIs
4. **Create alerts**: Set up alerting when SLOs are at risk (error budget burn rate)
5. **Review cadence**: Monthly SLO review to adjust thresholds based on real-world data
