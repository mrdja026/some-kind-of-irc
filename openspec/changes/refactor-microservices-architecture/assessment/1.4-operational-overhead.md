# 1.4 Operational Overhead Shifts

## Current State: Docker Compose

| Concern           | Current Tooling                   | Effort Level |
| ----------------- | --------------------------------- | ------------ |
| Orchestration     | `docker-compose.yml` (6 services) | Low          |
| Reverse proxy     | Caddy (ports 80/443)              | Low          |
| Database          | SQLite (file-based, no server)    | Trivial      |
| Caching           | None                              | N/A          |
| Rate limiting     | In-memory Python dict             | Trivial      |
| CI/CD             | Manual `docker compose up`        | Low          |
| Monitoring        | None (stdout logs only)           | Trivial      |
| Secret management | `.env.local` file                 | Low          |
| Scaling           | Not supported (single instance)   | N/A          |
| Service discovery | Docker DNS (service names)        | Automatic    |

**Total operational burden**: Low. One developer can manage the full stack with `docker compose up`.

---

## Target State: K3s + Microservices

### New Infrastructure Components

| Component              | Purpose                                      | Operational Cost |
| ---------------------- | -------------------------------------------- | ---------------- |
| **K3s**                | Single-node Kubernetes (Ubuntu LTS, systemd) | Medium           |
| **NGINX Ingress**      | API Gateway, path-based routing              | Medium           |
| **Argo CD**            | GitOps deployment, UI, sync management       | Medium           |
| **Redis**              | Rate limiting, caching, pub/sub              | Low-Medium       |
| **PostgreSQL**         | Auth Service database                        | Medium           |
| **Helm**               | Chart templating for K8s manifests           | Medium           |
| **Container Registry** | Store Docker images (local or remote)        | Low              |

### Overhead Comparison by Category

#### 1. Cluster Management (K3s)

| Task                | Current (Docker Compose) | Target (K3s)                               |
| ------------------- | ------------------------ | ------------------------------------------ |
| Install/setup       | `docker compose up`      | K3s install via systemd, kubeconfig setup  |
| Upgrade             | `docker compose pull`    | K3s binary upgrade, node drain             |
| Troubleshooting     | `docker logs`            | `kubectl logs`, `kubectl describe`, events |
| Resource monitoring | `docker stats`           | `kubectl top`, metrics-server              |
| Networking          | Docker bridge network    | CNI (Flannel), Services, Ingress           |

**Net change**: +Medium. K3s is lightweight but still requires Kubernetes knowledge.

#### 2. Deployment (Argo CD + Helm)

| Task                  | Current                 | Target                       |
| --------------------- | ----------------------- | ---------------------------- |
| Deploy new version    | `docker compose up -d`  | Git push → Argo CD auto-sync |
| Rollback              | Manual `docker compose` | Argo CD rollback (1-click)   |
| Config changes        | Edit `.env.local`       | Edit Helm values → Git push  |
| Multi-env support     | Not supported           | Helm values per environment  |
| Deployment visibility | Terminal output         | Argo CD UI dashboard         |

**Net change**: +Medium initially, but **reduces** long-term deployment friction. GitOps provides audit trail and rollback.

#### 3. Networking (NGINX Ingress)

| Task                | Current (Caddy)       | Target (NGINX Ingress)                |
| ------------------- | --------------------- | ------------------------------------- |
| Route configuration | `Caddyfile`           | Ingress YAML manifests                |
| TLS termination     | Caddy auto-HTTPS      | cert-manager + Let's Encrypt (future) |
| Path-based routing  | Caddy reverse_proxy   | Ingress path rules                    |
| Load balancing      | N/A (single instance) | K8s Service + Ingress                 |
| Health checks       | None                  | K8s readiness/liveness probes         |

**Net change**: +Low-Medium. NGINX Ingress is well-documented but requires YAML manifests.

#### 4. Data Layer (PostgreSQL + Redis)

| Task                  | Current                 | Target                                            |
| --------------------- | ----------------------- | ------------------------------------------------- |
| Database setup        | SQLite (auto-created)   | PostgreSQL pod + PVC + init SQL                   |
| Backups               | Copy `chat.db` file     | pg_dump CronJob or manual                         |
| Migrations            | SQLAlchemy `create_all` | Alembic migrations                                |
| Connection management | SQLite (no pooling)     | Connection pooling (pgbouncer or SQLAlchemy pool) |
| Cache setup           | None                    | Redis pod + PVC                                   |
| Cache management      | N/A                     | Redis CLI, key TTLs, eviction policy              |

**Net change**: +Medium. PostgreSQL and Redis require provisioning, monitoring, and backup strategies.

#### 5. CI/CD Pipeline

| Task         | Current                | Target                                  |
| ------------ | ---------------------- | --------------------------------------- |
| Build images | `docker compose build` | CI pipeline (GitHub Actions or similar) |
| Push images  | N/A (local only)       | Push to container registry              |
| Deploy       | Manual                 | Argo CD watches Git repo                |
| Test         | Manual `pytest`        | CI runs tests before merge              |
| Lint/format  | Manual                 | CI checks                               |

**Net change**: +Medium initially. Requires CI pipeline setup but automates quality gates.

#### 6. Monitoring & Observability

| Task          | Current                | Target                                   |
| ------------- | ---------------------- | ---------------------------------------- |
| Logs          | `docker logs` / stdout | `kubectl logs`, log aggregation (future) |
| Metrics       | None                   | Prometheus + metrics-server (future)     |
| Alerting      | None                   | Alertmanager (future)                    |
| Tracing       | None                   | OpenTelemetry (future)                   |
| Health checks | `/health` endpoint     | K8s probes + `/health`                   |

**Net change**: +Low for Phase 1 (basic `kubectl logs` + probes). Full observability stack is a future phase.

#### 7. Secret Management

| Task           | Current           | Target                          |
| -------------- | ----------------- | ------------------------------- |
| Store secrets  | `.env.local` file | K8s Secrets + Helm values       |
| Rotate secrets | Manual file edit  | K8s Secret update + pod restart |
| Access control | File permissions  | RBAC + namespace isolation      |

**Net change**: +Low. K8s Secrets are straightforward but require RBAC awareness.

---

## Multi-Service Ops Across Frameworks

The project runs services in three different frameworks:

| Service          | Framework | Runtime | Notes                                         |
| ---------------- | --------- | ------- | --------------------------------------------- |
| Backend monolith | FastAPI   | Python  | Main service, will shrink as services extract |
| Auth Service     | FastAPI   | Python  | New microservice (extracted)                  |
| AI Service       | FastAPI   | Python  | New microservice (extracted)                  |
| media-storage    | Flask     | Python  | Existing separate service                     |
| data-processor   | Django    | Python  | Existing separate service                     |
| Frontend         | TanStack  | Node.js | SPA served by Nginx                           |

**Operational impact**: All Python services share similar Dockerfile patterns and dependency management (`requirements.txt` / `pip`). The main overhead is managing 5+ Kubernetes Deployments instead of 6 Docker Compose services.

---

## Summary: Overhead Delta

| Category           | Current Effort | Target Effort | Delta       |
| ------------------ | -------------- | ------------- | ----------- |
| Cluster management | Trivial        | Medium        | +Medium     |
| Deployment         | Low            | Medium        | +Low        |
| Networking         | Low            | Medium        | +Low-Med    |
| Data layer         | Trivial        | Medium        | +Medium     |
| CI/CD              | Low            | Medium        | +Medium     |
| Monitoring         | Trivial        | Low           | +Low        |
| Secret management  | Low            | Low           | Neutral     |
| **Total**          | **Low**        | **Medium**    | **+Medium** |

## Key Takeaway

The migration adds **medium operational overhead** primarily from K3s cluster management, PostgreSQL/Redis provisioning, and Helm/Argo CD setup. However, this overhead is front-loaded (one-time setup) and pays off through:

- Automated GitOps deployments (Argo CD)
- Independent service scaling
- Better fault isolation
- Production-ready infrastructure patterns

The team needs Kubernetes familiarity. K3s minimizes the learning curve compared to full K8s, but `kubectl`, Helm, and Ingress YAML are required skills.
